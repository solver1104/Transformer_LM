# Transformer_LM
Implements a Transformer architecture from scratch (a downscaled GPT-2), and code to train it for auto-regressive language modelling. Currently uses character level tokenization, looking into subword tokenization.
