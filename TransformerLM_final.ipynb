{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "'''\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "head_sz = 8\n",
        "embed_sz = 32\n",
        "token_unique_n = 8\n",
        "block_sz = 4\n",
        "\n",
        "head_sz = head_sz\n",
        "embed_sz = embed_sz\n",
        "head_n = embed_sz / head_sz\n",
        "block_sz = block_sz\n",
        "\n",
        "char_embed = nn.Embedding(token_unique_n, embed_sz)\n",
        "pos_embed = nn.Embedding(token_unique_n, embed_sz)\n",
        "\n",
        "q = nn.Linear(embed_sz, head_sz, bias = False)\n",
        "k = nn.Linear(embed_sz, head_sz, bias = False)\n",
        "v = nn.Linear(embed_sz, head_sz, bias = False)\n",
        "softmax = nn.Softmax(dim = 1)\n",
        "mask = torch.ones((block_sz, block_sz)).triu()\n",
        "\n",
        "inp = torch.tensor([[2, 7, 0, 1], [1, 1, 1, 3], [7, 0, 0, 1], [5, 5, 0, 1], [0, 0, 2, 6]])\n",
        "\n",
        "###### Forward\n",
        "\n",
        "data = torch.stack([torch.stack([char_embed(i[j]) + pos_embed(torch.tensor(j)) for j in range(len(i))]) for i in inp])\n",
        "\n",
        "query = q(data)\n",
        "key = k(data)\n",
        "value = v(data)\n",
        "\n",
        "attention = softmax(query @ key.transpose(1,2) / math.sqrt(head_sz) * mask.repeat((len(inp), 1, 1))) @ value\n",
        "\n",
        "'''\n",
        "\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "a = torch.tril(a) + torch.triu(torch.ones_like(a) * float('-inf'), diagonal = 1)\n",
        "\n",
        "print(a)"
      ],
      "metadata": {
        "id": "oUqP8_fijCUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae4402e-4625-42e3-d558-f677b97a0c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., -inf, -inf],\n",
            "        [4., 5., -inf],\n",
            "        [7., 8., 9.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sECk2e9e0I7Q"
      },
      "outputs": [],
      "source": [
        "# Testing vanilla Transformer architecture for Autoregressive Language Generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGvqfTx6vGS9",
        "outputId": "50f474a6-511f-4351-ed29-de4b75b4d582"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext import datasets\n",
        "from torch import nn\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "8RjZg5Yl02fQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture hyperparams\n",
        "special_char = ['\\t', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '[', '\\\\', ']', '_', '{', '|', '}', \"\\n\"]\n",
        "\n",
        "num_chars = 93\n",
        "train_batch_sz = 32\n",
        "macro_batch_sz = 8 # For Gradient Accumulation to train with larger batch sizes without overflowing memory\n",
        "truncate_len = 512\n",
        "\n",
        "head_sz = 32\n",
        "head_n = 12\n",
        "embed_sz = 324\n",
        "layer_n = 12\n",
        "fcnn_sz = 1536\n",
        "# dropout_p = 0.3\n",
        "\n",
        "cycles = 100000\n",
        "\n",
        "# learning rate\n",
        "min_lr = 6e-5\n",
        "max_lr = 6e-4\n",
        "\n",
        "# Epochs to train model for (each epoch loops through the corpus once)\n",
        "epochs = 50\n",
        "\n",
        "# Convert character to index\n",
        "def char_index(x):\n",
        "  if ord(x) < 91 and ord(x) > 64:\n",
        "    return ord(x) - 65\n",
        "  if ord(x) < 123 and ord(x) > 96:\n",
        "    return ord(x) - 71\n",
        "  return special_char.index(x) + 52\n",
        "\n",
        "# Filter characters in input data for relevant characters\n",
        "def keep_char(x):\n",
        "  return (ord(x) < 91 and ord(x) > 64) or (ord(x) < 123 and ord(x) > 96) or x in special_char\n",
        "\n",
        "# Convert index to character\n",
        "def index_char(ind):\n",
        "  if ind < 26:\n",
        "    return chr(ind + 65)\n",
        "  if ind < 52:\n",
        "    return chr(ind + 71)\n",
        "  return special_char[ind - 52]"
      ],
      "metadata": {
        "id": "1COP3CKW047j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f'Using Device {device}')"
      ],
      "metadata": {
        "id": "MqfU_1Gn1lKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c10cfa-69a1-4727-f2ea-1f187444a92b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  def __init__(self, head_sz, token_unique_n, embed_sz):\n",
        "    super().__init__()\n",
        "    self.head_sz = head_sz\n",
        "    self.embed_sz = embed_sz\n",
        "\n",
        "    self.q = nn.Linear(embed_sz, head_sz, bias = False)\n",
        "    self.k = nn.Linear(embed_sz, head_sz, bias = False)\n",
        "    self.v = nn.Linear(embed_sz, head_sz, bias = False)\n",
        "\n",
        "  def forward(self, inp):\n",
        "    query = self.q(inp)\n",
        "    key = self.k(inp)\n",
        "    value = self.v(inp)\n",
        "\n",
        "    attention = query @ key.transpose(1,2) / math.sqrt(self.head_sz)\n",
        "    attention = torch.tril(attention) + torch.triu(torch.ones_like(attention) * float('-inf'), diagonal = 1)\n",
        "    attention = nn.functional.softmax(attention, dim = -1) @ value\n",
        "    return attention\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, head_sz, head_n, token_unique_n, embed_sz):\n",
        "    super().__init__()\n",
        "    self.head_sz = head_sz\n",
        "    self.embed_sz = embed_sz\n",
        "    self.head_n = head_n\n",
        "\n",
        "    self.heads = nn.ModuleList([SelfAttentionHead(head_sz, token_unique_n, embed_sz).to(device) for _ in range(int(head_n))])\n",
        "    self.out = nn.Linear(int(self.head_n) * head_sz, embed_sz)\n",
        "\n",
        "  def forward(self, inp):\n",
        "    return self.out(torch.cat([head(inp) for head in self.heads], dim = -1))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, head_sz, head_n, token_unique_n, embed_sz, fcnn_sz):\n",
        "    super().__init__()\n",
        "    self.head_sz = head_sz\n",
        "    self.embed_sz = embed_sz\n",
        "    self.head_n = head_n\n",
        "\n",
        "    self.attention = MultiHeadSelfAttention(head_sz, head_n, token_unique_n, embed_sz).to(device)\n",
        "    self.layernorm1 = nn.LayerNorm(embed_sz)\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(embed_sz, fcnn_sz),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(fcnn_sz, embed_sz)\n",
        "    )\n",
        "    self.layernorm2 = nn.LayerNorm(embed_sz)\n",
        "\n",
        "  def forward(self, inp):\n",
        "    inp = inp + self.attention(self.layernorm1(inp))\n",
        "    inp = inp + self.fc(self.layernorm2(inp))\n",
        "\n",
        "    return inp\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, head_sz, head_n, token_unique_n, block_sz, embed_sz, layer_n, fcnn_sz):\n",
        "    super().__init__()\n",
        "    self.char_embed = nn.Embedding(token_unique_n, embed_sz)\n",
        "    self.pos_embed = nn.Embedding(block_sz, embed_sz)\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      *[TransformerBlock(head_sz, head_n, token_unique_n, embed_sz, fcnn_sz).to(device) for _ in range(layer_n)]\n",
        "    )\n",
        "\n",
        "    self.out_layernorm = nn.LayerNorm(embed_sz),\n",
        "    self.out = nn.Linear(embed_sz, token_unique_n, bias = False)\n",
        "\n",
        "    self.char_embed.weight = self.out.weight # Weight tie the vocabulary embedding weights and the output projection weights\n",
        "\n",
        "  def forward(self, inp):\n",
        "    data = torch.stack([torch.stack([self.char_embed(i[j]) + self.pos_embed(torch.tensor(j).to(device)) for j in range(len(i))]) for i in inp])\n",
        "    data = self.model(data)\n",
        "    return self.out(data)\n"
      ],
      "metadata": {
        "id": "FoiSztlR1qqa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Initialization\n",
        "def init_weights(m):\n",
        "  if type(m) is nn.Linear:\n",
        "    nn.init.xavier_normal_(m.weight)\n",
        "    if m.bias is not None:\n",
        "      torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "  if type(m) is nn.Embedding:\n",
        "    nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "\n",
        "def construct_param_groups(m):\n",
        "  if type(m) is nn.Linear and m.out_features != num_chars:\n",
        "    weight_decay.append(m.weight)\n",
        "    if m.bias is not None:\n",
        "      no_weight_decay.append(m.bias)\n",
        "  if type(m) is nn.Embedding:\n",
        "    no_weight_decay.append(m.weight)\n",
        "  if type(m) is nn.LayerNorm:\n",
        "    no_weight_decay.append(m.weight)\n",
        "\n",
        "weight_decay = []\n",
        "no_weight_decay = []\n",
        "\n",
        "optim_groups = [\n",
        "    {\"params\": weight_decay, \"weight_decay\": 0.1},\n",
        "    {\"params\": no_weight_decay, \"weight_decay\": 0}\n",
        "]\n",
        "\n",
        "model = TransformerModel(head_sz, head_n, num_chars, truncate_len, embed_sz, layer_n, fcnn_sz).to(device)\n",
        "model.apply(init_weights)\n",
        "model.apply(construct_param_groups)\n",
        "opt = torch.optim.AdamW(optim_groups, lr = 3e-5, betas = (0.9, 0.95))\n",
        "\n",
        "print(f'Parameter Number: {sum(p.numel() for p in model.parameters())}')"
      ],
      "metadata": {
        "id": "O9v28k2_1y7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def construct_param_groups(m):\n",
        "  if type(m) is nn.Linear and m.out_features != num_chars:\n",
        "    weight_decay.append(m.weight)\n",
        "    if m.bias is not None:\n",
        "      no_weight_decay.append(m.bias)\n",
        "  if type(m) is nn.Embedding:\n",
        "    no_weight_decay.append(m.weight)\n",
        "  if type(m) is nn.LayerNorm:\n",
        "    no_weight_decay.append(m.weight)\n",
        "\n",
        "weight_decay = []\n",
        "no_weight_decay = []\n",
        "\n",
        "optim_groups = [\n",
        "    {\"params\": weight_decay, \"weight_decay\": 0.1},\n",
        "    {\"params\": no_weight_decay, \"weight_decay\": 0}\n",
        "]\n",
        "\n",
        "model = TransformerModel(head_sz, head_n, num_chars, truncate_len, embed_sz, layer_n, fcnn_sz).to(device)\n",
        "model.apply(construct_param_groups)\n",
        "opt = torch.optim.AdamW(optim_groups, lr = 2e-6, betas = (0.9, 0.95))\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/My Drive/TransformerLM/state.pth\"))\n",
        "opt.load_state_dict(torch.load(\"/content/gdrive/My Drive/TransformerLM/opt.pth\"))\n",
        "\n",
        "print(f'Parameter Number: {sum(p.numel() for p in model.parameters())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qiZAirTb73E",
        "outputId": "b5aa0a28-2ddc-4d2c-f55c-17740c789ea2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Parameter Number: 18153684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data (EnWik9 database in torchtext)\n",
        "training_dataloader = iter(datasets.EnWik9(root = \"data\"))\n",
        "\n",
        "# Aggregate Data into tensor\n",
        "data = \"\"\n",
        "line_n = 0\n",
        "\n",
        "with tqdm.tqdm(total = 3000000) as t:\n",
        "  for x in training_dataloader:\n",
        "    if line_n == 3000000:\n",
        "      break\n",
        "    for char in x:\n",
        "      if keep_char(char):\n",
        "        data += char\n",
        "\n",
        "    data += \"\\n\"\n",
        "\n",
        "    line_n += 1\n",
        "    t.update(1)"
      ],
      "metadata": {
        "id": "B0n1jKBtyPqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4967af-e060-4f79-c3c5-2431f5ffa15e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000000/3000000 [07:10<00:00, 6974.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample\n",
        "def sample(sz):\n",
        "  idx = random.randint(0, len(data) - sz - 2)\n",
        "  # idx = 0\n",
        "  return [char_index(data[i]) for i in range(idx, idx + sz + 1)]\n",
        "\n",
        "# Generate batched sample\n",
        "def get_batch(batch_sz):\n",
        "  return [sample(truncate_len) for _ in range(batch_sz)]\n",
        "\n",
        "def prompt_sample(prompt):\n",
        "  return [char_index(i) for i in prompt]"
      ],
      "metadata": {
        "id": "RXxVmD_M15R7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling hyperparameters\n",
        "sample_model = True\n",
        "sample_from_prompt = False\n",
        "prompt = \"The most interesting \"\n",
        "\n",
        "sample_topk = 5\n",
        "sample_freq = 50\n",
        "\n",
        "# Number of characters to sample from model each testing cycle\n",
        "sample_length = 1024\n",
        "sample_context = 64\n",
        "sample_temp = 0.7\n",
        "\n",
        "# Saving frequency\n",
        "save_freq = 25\n",
        "\n",
        "def train_cycle():\n",
        "  model.zero_grad()\n",
        "\n",
        "  for train_cycle in range(cycles):\n",
        "    if train_cycle % sample_freq == 0 and sample_model:\n",
        "      test_cycle()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print(f'Train Cycle {train_cycle}')\n",
        "    loss = 0\n",
        "    accumulated_loss = 0\n",
        "\n",
        "    for _ in range(macro_batch_sz):\n",
        "      data = np.array(get_batch(train_batch_sz))\n",
        "      expected = torch.tensor(data[:, 1:]).to(device)\n",
        "      data = torch.tensor(data[:, :-1]).to(device)\n",
        "      output = model(data)\n",
        "\n",
        "      loss = nn.functional.cross_entropy(torch.reshape(output, (-1, num_chars)), torch.reshape(expected, (-1, ))) / macro_batch_sz\n",
        "      accumulated_loss += loss\n",
        "      loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    opt.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    print(f'Iteration Loss: {round(accumulated_loss.item(), 3)}')\n",
        "\n",
        "    if train_cycle % save_freq == save_freq - 1:\n",
        "      save_model()\n",
        "\n",
        "def test_cycle():\n",
        "  print('================================================== Test Cycle ==================================================')\n",
        "  model.eval()\n",
        "\n",
        "  # Generate a sample to serve as context for the model\n",
        "  if sample_from_prompt:\n",
        "    context = [prompt_sample(prompt)]\n",
        "  else:\n",
        "    context = [sample(sample_context)]\n",
        "\n",
        "  for i in context[0]:\n",
        "    print(index_char(i), end = '')\n",
        "\n",
        "  for i in range(sample_length):\n",
        "    output_distribution = model(torch.tensor(context).to(device))[0][-1]\n",
        "    top_chars = torch.topk(output_distribution, sample_topk)\n",
        "    sampled_char = top_chars[1][list(torch.utils.data.WeightedRandomSampler(nn.functional.softmax(top_chars[0] * sample_temp, dim = 0), 1))[0]].item()\n",
        "\n",
        "    context[0].append(sampled_char)\n",
        "    if len(context[0]) > truncate_len:\n",
        "      context[0] = context[0][1:]\n",
        "    print(index_char(sampled_char), end = '')\n",
        "\n",
        "  print()\n",
        "  print('================================================================================================================')\n",
        "\n",
        "def save_model():\n",
        "  PATH = \"/content/gdrive/My Drive/TransformerLM/state.pth\"\n",
        "  torch.save(model.state_dict(), PATH)\n",
        "  PATH = \"/content/gdrive/My Drive/TransformerLM/opt.pth\"\n",
        "  torch.save(opt.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "mAp9Ro0FgwoB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in opt.param_groups:\n",
        "  g['lr'] = 3e-5\n",
        "\n",
        "print(opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Vf78Y_Dhyk",
        "outputId": "f139cf1e-e6c4-4a4a-933a-ebca194c1d24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 3e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.1\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 3e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training cycles\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  train_cycle()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCvBn1DID4Fq",
        "outputId": "8c472811-c4e9-44b8-a2f4-7c84111bf3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "================================================== Test Cycle ==================================================\n",
            " the maker welds a thin rod to the end of the blade at the crossgored ones of the side, which would never through the parents of allight at time.\n",
            "\n",
            "The third is another offee of a [[policy]] only invalid body, while these criticizer and the [[southwart]] to true the counservational asks, warmly the televised one. Intended, in this, as they also well trying in the penant to pen to the south ones, withdrings were tryings. Truck is survival. In people a spen with someone's truck assumed. The peninuality cruce, this would shoot be reply overlow the city at their, as to still of.\n",
            "\n",
            "Trucking a single asks with polemest took as a cashing offershiving personnallization in [[1945 in the Control|Control]] issued in [[1949]], though would be a prophet on in the society, and the country. Television islet, the penanth spenten of it, we consided the so"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "PATH = \"/content/gdrive/My Drive/TransformerLM/state.pth\"\n",
        "torch.save(model.state_dict(), PATH)\n",
        "PATH = \"/content/gdrive/My Drive/TransformerLM/opt.pth\"\n",
        "torch.save(opt.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "Sfk8dEvDDpN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c07178-c49c-4d19-b055-93ed9613bdc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}